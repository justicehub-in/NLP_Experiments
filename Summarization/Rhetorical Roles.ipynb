{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03face28",
   "metadata": {},
   "source": [
    "# Rhetorical Roles\n",
    "\n",
    "In [NER](https://github.com/d-saikrishna/NLP/blob/master/NER/NER_Learnings.ipynb), we saw how every word or a phrase can be identified as an entity (ORG, PERSON etc.,). But sometimes, the entire statement or a group of statements may mean something (FACT, ANALYSIS etc.,) Identification of the rhetorical roles is another NLP Task.\n",
    "\n",
    "## Use-Cases:\n",
    "1. Extractive Summarization of Documents: Extracting salient sentences that would summarize the document.\n",
    "2. Abstractive Summarization od Documents: Generating concise text summaries.\n",
    "3. Prediction (of judgements etc)\n",
    "\n",
    "These use-cases can be done without RR also. But use of RR in model input has shown to increase the performance of summarisation/prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4580e",
   "metadata": {},
   "source": [
    "# Opennyai RR\n",
    "    \n",
    "12 Rhetorical roles are defined by Opennyai. Moreover, Opennyai defined RRs at sentence level. Therefore, every sentence of a cout judgement could be wither NONE or: PREAMBLE, FACT, RULING BY LOWER COURT, ISSUE, ARGUMENT_PETITIONER, ARGUMENT_RESPONDENT, ANALYSIS, STATUTE, PRECEDENT_RELIED, PRECEDENT_NOT_RELIED, RATIO, RULING BY PRESENT COURT. It can be understood as a **Multi-class classification problem**. Definitions to these RR classes can be found here: [Link](https://opennyai.readthedocs.io/en/latest/rr/rr_structure.html)\n",
    "\n",
    "\n",
    "Currently, not every class/RR is classified well. While PREAMBLE, ISSUE, NONE sentences are well identified. PRECEDENT_NOT_RELIED, RATIO and ARGUMENT_RESPONDENT are poorly identified. **The overall F1 Score of the model is 0.79.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642c05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opennyai import RhetoricalRolePredictor\n",
    "from opennyai.utils import Data\n",
    "from opennyai import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff6d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Sample court judgements\n",
    "text1 = open('SampleTexts/sample_judgment1.txt').read()\n",
    "text2 = open('SampleTexts/sample_judgment2.txt').read()\n",
    "\n",
    "# you can also load your text files directly into this\n",
    "texts_to_process = [text1, text2]\n",
    "\n",
    "# create Data object for data  preprocessing before running ML models\n",
    "data = Data(texts_to_process, preprocessing_nlp_model='en_core_web_trf')\n",
    "\n",
    "#Other pre-processing models available are: en_core_web_md, en_core_web_sm (fastest but less accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04599ce",
   "metadata": {},
   "source": [
    "The following pre-processing steps takes place with the above code:\n",
    "\n",
    "1. Separating preamble from judgment text\n",
    "2. Sentence splitting of judgment text\n",
    "3. Convert upper case words in preamble to title case\n",
    "4. Replace newline characters within a sentence with space in judgment text\n",
    "\n",
    "More on pre-processing: [Link](https://opennyai.readthedocs.io/en/latest/preprocessing/preprocessing.html) (chunks etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a4b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have access to GPU then set this to True else False\n",
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522a629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107778f5a05e45628d0cdaeed6dd895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: \"https://storage.googleapis.com/indianlegalbert/OPEN_SOURCED_FILES/Rhetorical_Role_Benchmark/Model/model.pt\" to /home/krishna/.opennyai/rhetoricalrole/model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c5b2b2205a4cb9813f36a0bb2ceaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/998M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ddc248b45d4f8793020e822a0439f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d51a2aaefb4c508191a083c6041ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc05e6d3ee14371bd191699dcba8902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline(components=['Rhetorical_Role'], use_gpu=use_gpu, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0985c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:17<00:00,  8.51s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:28<00:00, 14.36s/it]\n"
     ]
    }
   ],
   "source": [
    "results = pipeline(data)\n",
    "\n",
    "json_result_doc_1 = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7ca5e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 3693,\n",
       " 'end': 3804,\n",
       " 'text': 'Being aggrieved thereby, the appellants filed an appeal under Section 37 of the 1996 Act before the High Court.',\n",
       " 'labels': ['FAC'],\n",
       " 'id': '3560773625804cd987910559ac33ab22_32'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result_doc_1['annotations'][32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7b5dd",
   "metadata": {},
   "source": [
    "# References\n",
    "1. [EkStep RR](https://github.com/Legal-NLP-EkStep/rhetorical-role-baseline)\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e5519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
