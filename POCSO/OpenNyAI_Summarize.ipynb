{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d9ee5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "from opennyai import Pipeline\n",
    "from opennyai.utils import Data\n",
    "from opennyai.ner import get_unique_provision_count\n",
    "\n",
    "import urllib\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b66bc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cases_cino = pd.read_csv('Results/POCSO_Filter2.csv')['cino'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e1df7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all POCSO judgements in Assam\n",
    "judgement_paths_assam = []\n",
    "path =\"Assam\"\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if(file.endswith(\".txt\")):\n",
    "            judgement_paths_assam.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e18ed9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 51 filtered POCSO judgements in Assam\n",
    "judgements_filtered = []\n",
    "for judgement in judgement_paths_assam:\n",
    "    #Get Case_ID \n",
    "    case_id = judgement.split(r'/')[-1].split('.txt')[0]\n",
    "    if case_id in filtered_cases_cino:\n",
    "        judgements_filtered.append(judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b533e285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judgements_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d361e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da94a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive Summarisation\n",
    "def opennyai_extractive_summary(judgement_path):\n",
    "    judgement_file = open(judgement_path)\n",
    "    judgement_text = judgement_file.read()\n",
    "    #Get Case_ID \n",
    "    case_id = judgement_path.split(r'/')[-1].split('.txt')[0]\n",
    "    \n",
    "    # load your text files directly into this\n",
    "    texts_to_process = [judgement_text]\n",
    "    # create Data object for data  preprocessing before running ML models\n",
    "    data = Data(texts_to_process, preprocessing_nlp_model='en_core_web_trf')\n",
    "    \n",
    "    pipeline = Pipeline(components=['Rhetorical_Role', 'Summarizer'], use_gpu=False, verbose=True,\n",
    "                    summarizer_summary_length=0.0)\n",
    "    \n",
    "    results = pipeline(data)\n",
    "    \n",
    "    json_result_doc_1 = results[0]\n",
    "    summaries_doc_1 = results[0]['summary']\n",
    "    judgement_file.close()\n",
    "    \n",
    "    with open(\"POCSO_Experiment_Judgements/\"+case_id+\"_summary.json\", \"w\") as outfile:\n",
    "        outfile.write(json.dumps(summaries_doc_1, indent=4))\n",
    "    return summaries_doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8573c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASNG010008652018\n",
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:07<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASNG010018122018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.03s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASNG010018122018\n",
      "ASSN010001342017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.55s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "No Provisions found in Decision\n",
      "NER Done\n",
      "ASSN010004782017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010004782017\n",
      "ASSN010013622017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:24<00:00, 24.15s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:37<00:00, 37.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010013622017\n",
      "ASSN010005202019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010005202019\n",
      "ASSN070002372019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.06s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070002372019\n",
      "ASSN070000452017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.77s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070000452017\n",
      "ASSN070000182017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070000182017\n",
      "ASSN070002502018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.35s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070002502018\n",
      "ASSN010000912018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.38s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:44<00:00, 44.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010000912018\n",
      "ASSN070003322018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070003322018\n",
      "ASSN010013062017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.13s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010013062017\n",
      "ASSN010002092017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.07s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010002092017\n",
      "ASSN010003222017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.67s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:43<00:00, 43.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSN010002382019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.25s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "c36ccf3e9937e248b14511da7dcb0edbe05375e361c6684c61a4a27a8686e921.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010002382019\n",
      "ASSN010015512017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.73s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Decision\n",
      "NER Done\n",
      "ASSN010011552017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.90s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:24<00:00, 24.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSN010017512018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.69s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:25<00:00, 25.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSN010011242017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.71s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSN070004442019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070004442019\n",
      "ASSN010000522018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.47s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010000522018\n",
      "ASSN010012862017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.57s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:21<00:00, 21.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010012862017\n",
      "ASSN070005512018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070005512018\n",
      "ASSN010004422017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010004422017\n",
      "ASSN070000642019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070000642019\n",
      "ASSV010001962017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.62s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "No Provisions found in Decision\n",
      "NER Done\n",
      "ASSV090001682017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.09s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV090001682017\n",
      "ASSV090001602017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.08s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV090001602017\n",
      "ASSV010003612019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "1a5602f8fe335a781fa0804654816314551eeb4cb01d495f89b455039a0bd1ef.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010003612019\n",
      "ASSV010005512019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:23<00:00, 23.25s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:48<00:00, 48.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010008752017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.90s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010006662017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:21<00:00, 21.60s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:36<00:00, 36.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSV010010952017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.33s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010003112017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.55s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010003112017\n",
      "ASSV010012372017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.37s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010012372017\n",
      "ASSV010013012018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.92s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:30<00:00, 30.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010001942019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.91s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSV010011632017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSV010014162017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.80s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "8fd94ae592f700586f8611ec9b6233ec4e36dcd080c28fde3b813cdf89b6c1eb.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010014162017\n",
      "ASBR010018052018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010018052018\n",
      "ASBR010009782017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010009782017\n",
      "ASBR010003392018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.24s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010003392018\n",
      "ASBR080002302019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  5.00s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASBR010001322018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.83s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASBR010013262018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASBR010003042019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.75s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010003042019\n",
      "ASBR080001152019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR080001152019\n",
      "ASBR010011522019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.68s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "553ad747763a4dd5166912951341a7c667aaaea97d338c2ac8d9b3f1b7120bac.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010011522019\n",
      "ASBR010018192018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.93s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010018192018\n",
      "ASBR010014222017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010014222017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create summaries of all 51 judgements selected\n",
    "for judgement_path in judgements_filtered:\n",
    "    case_id = judgement_path.split(r'/')[-1].split('.txt')[0]\n",
    "    print(case_id)\n",
    "    if 'POCSO_Experiment_Judgements/'+case_id+'_summary.json' in glob.glob(\"POCSO_Experiment_Judgements/*json\"):\n",
    "        pass\n",
    "    else:\n",
    "        summaries_doc_1 = opennyai_extractive_summary(judgement_path)\n",
    "    print('Summarisation Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318528b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(\"POCSO_Experiment_Judgements/*json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa6d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pocso_provisions(text):\n",
    "    '''\n",
    "    input param: text: Judgement text in string format.\n",
    "    \n",
    "    return: List of all POCSO Provisions found in the judgement text.\n",
    "    '''\n",
    "    pattern = '\\d+ of pocso'\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    pattern = '\\d+ of p.o.c.s.o'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of posco'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of the pocso'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of the posco'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of prot'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of the prot'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pattern2 = '\\d+'\n",
    "    provisions_found = []\n",
    "    for match in matches:\n",
    "        provisions_found.append(re.findall(pattern2, match))\n",
    "    provisions_found = list(chain.from_iterable(provisions_found))\n",
    "    \n",
    "    return provisions_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45bfe32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "provisions_dfs = []\n",
    "\n",
    "for summary in glob.glob(\"POCSO_Experiment_Judgements/*json\")[:]:\n",
    "    case_id = summary.split('/')[-1].split('_')[0]\n",
    "    with open(summary) as openfile:\n",
    "        # Reading from json file\n",
    "        summaries_doc_1 = json.load(openfile)\n",
    "        \n",
    "    # Extract POCSO provisions from PREAMBLE summary\n",
    "    provisions_preamble = find_pocso_provisions(summaries_doc_1['PREAMBLE'])\n",
    "    \n",
    "    # Extract POCSO provisions from decision summary\n",
    "    provisions_decision = find_pocso_provisions(summaries_doc_1['decision'])\n",
    "    \n",
    "    # Extract POCSO provisions from facts summary\n",
    "    try:\n",
    "        provisions_facts = find_pocso_provisions(summaries_doc_1['facts'])\n",
    "    except:\n",
    "        provisions_facts = []\n",
    "    \n",
    "    # Extract POCSO provisions from issue summary\n",
    "    try:\n",
    "        provisions_issue = find_pocso_provisions(summaries_doc_1['issue'])\n",
    "    except:\n",
    "        provisions_issue = []\n",
    "    \n",
    "    # Extract POCSO provisions from ANALYSIS summary\n",
    "    provisions_analysis = find_pocso_provisions(summaries_doc_1['ANALYSIS'])\n",
    "    \n",
    "    provisions_df = pd.DataFrame([[case_id],\n",
    "                                  [set(provisions_preamble)],\n",
    "                                  [set(provisions_decision)],\n",
    "                                 [set(provisions_facts)],\n",
    "                                  [set(provisions_issue)],\n",
    "                                  [set(provisions_analysis)]]\n",
    "                                ).T\n",
    "    \n",
    "    provisions_df.columns = ['case_id', 'provision_preamble', 'provision_decision',\n",
    "                             'provisions_facts', 'provisions_issue', 'provisions_analysis']\n",
    "    \n",
    "    provisions_dfs.append(provisions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "138c4150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>provision_preamble</th>\n",
       "      <th>provision_decision</th>\n",
       "      <th>provisions_facts</th>\n",
       "      <th>provisions_issue</th>\n",
       "      <th>provisions_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASSV010003112017</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{29}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASSN010002092017</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSN070000182017</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASSV090001682017</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASSN070000452017</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_id provision_preamble provision_decision provisions_facts  \\\n",
       "0  ASSV010003112017                {8}                {8}              {8}   \n",
       "1  ASSN010002092017                {4}                {4}              {4}   \n",
       "2  ASSN070000182017                {8}                 {}              {8}   \n",
       "3  ASSV090001682017                {4}                 {}               {}   \n",
       "4  ASSN070000452017                {8}                 {}              {8}   \n",
       "\n",
       "  provisions_issue provisions_analysis  \n",
       "0               {}                {29}  \n",
       "1              {4}                  {}  \n",
       "2               {}                  {}  \n",
       "3               {}                 {4}  \n",
       "4               {}                  {}  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pocso_provisions_opennyai = pd.concat(provisions_dfs).reset_index(drop=True)\n",
    "pocso_provisions_opennyai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f7db1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the POCSO provisions as provided in the metadata (eCourts)\n",
    "metadata_sections_dfs = []\n",
    "for judgement in judgements_filtered:\n",
    "    # Get Case_ID \n",
    "    case_id = judgement.split(r'/')[-1].split('.txt')[0]\n",
    "    \n",
    "    # Get Provisions and Statute information from parsed meta data\n",
    "    folder_path = judgement.split(r'/')\n",
    "    folder_path.pop()\n",
    "    case_metadata_json = r\"/\".join(folder_path)+\"/\"+case_id+\"_parsed.json\"\n",
    "    with open(case_metadata_json) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    metadata_sections_df = pd.DataFrame(data['acts'])\n",
    "    metadata_sections_df.columns = ['statute','provision_metadata']\n",
    "    metadata_sections_df['case_id'] = case_id\n",
    "    metadata_sections_dfs.append(metadata_sections_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32856519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Protection of Children from Sexual Offence Act 2012',\n",
       "       'Protection of Children from Sexual Offences', 'Indian Penal Code',\n",
       "       'Pocso 2012', 'Protection of Children from sexual offence Act',\n",
       "       '1Indian Penal Code',\n",
       "       '5Protection of children from sexual offence act 2012',\n",
       "       'Protection of Children from Sexual Offences Act,2012',\n",
       "       \"Protection of Children from Sexual Offences Act'2012\",\n",
       "       'Information Technology Act'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pocso_provisions_metadata = pd.concat(metadata_sections_dfs)\n",
    "pocso_provisions_metadata.statute.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a0153375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krishna/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "#Considering only POCSO Provisions on metadata\n",
    "pocso_provisions_metadata = pocso_provisions_metadata[~pocso_provisions_metadata.statute.isin(['Indian Penal Code',\n",
    "                                                                           '1Indian Penal Code',\n",
    "                                                                           'Information Technology Act'])]\n",
    "pocso_provisions_metadata.statute = 'POCSO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b332eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statute</th>\n",
       "      <th>provision_metadata</th>\n",
       "      <th>case_id</th>\n",
       "      <th>provision_preamble</th>\n",
       "      <th>provision_decision</th>\n",
       "      <th>provisions_facts</th>\n",
       "      <th>provisions_issue</th>\n",
       "      <th>provisions_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POCSO</td>\n",
       "      <td>[6]</td>\n",
       "      <td>ASNG010008652018</td>\n",
       "      <td>{6}</td>\n",
       "      <td>{6}</td>\n",
       "      <td>{6, 4}</td>\n",
       "      <td>{6}</td>\n",
       "      <td>{29}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POCSO</td>\n",
       "      <td>[366(A), ]</td>\n",
       "      <td>ASNG010018122018</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{4, 29}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POCSO</td>\n",
       "      <td>[4]</td>\n",
       "      <td>ASSN010001342017</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{8, 4}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POCSO</td>\n",
       "      <td>[6]</td>\n",
       "      <td>ASSN010004782017</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{6, 4}</td>\n",
       "      <td>{4}</td>\n",
       "      <td>{4, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POCSO</td>\n",
       "      <td>[4]</td>\n",
       "      <td>ASSN010013622017</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{4, 8}</td>\n",
       "      <td>{8}</td>\n",
       "      <td>{8}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  statute provision_metadata           case_id provision_preamble  \\\n",
       "0   POCSO                [6]  ASNG010008652018                {6}   \n",
       "1   POCSO         [366(A), ]  ASNG010018122018                {4}   \n",
       "2   POCSO                [4]  ASSN010001342017                {8}   \n",
       "3   POCSO                [6]  ASSN010004782017                {4}   \n",
       "4   POCSO                [4]  ASSN010013622017                {8}   \n",
       "\n",
       "  provision_decision provisions_facts provisions_issue provisions_analysis  \n",
       "0                {6}           {6, 4}              {6}                {29}  \n",
       "1                 {}              {4}              {4}             {4, 29}  \n",
       "2                 {}           {8, 4}              {8}                 {8}  \n",
       "3                 {}           {6, 4}              {4}              {4, 3}  \n",
       "4                {8}           {4, 8}              {8}                 {8}  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pocso_provisions = pocso_provisions_metadata.merge(pocso_provisions_opennyai, on='case_id')\n",
    "pocso_provisions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "83be268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pocso_provisions.to_csv('Results/pocso_provisions_51judgements.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
